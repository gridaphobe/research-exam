#+TITLE: Research Exam
#+AUTHOR:
#+DATE:
#+OPTIONS: toc:nil texht:t ':t H:4
#+LATEX_CLASS: sigplanconf
#+LATEX_CLASS_OPTIONS: [blockstyle,preprint,nocopyrightspace]
#+LATEX_HEADER: \authorinfo{Eric Seidel}
# #+LATEX_HEADER: \usepackage{courier}
# #+BIBLIOGRAPHY: bibliography plain

# FOCUS: Automatic test-case generation
# 1. enumerate inputs
# 2. enumerate code-paths
# 3. translate counterexamples from static analysis into tests

# Systems to cover
# - QuickCheck
# - SmallCheck
# - Korat (/ TestEra)
# - CUTE / DART / PEX
# - jCrasher / Check'n'Crash
# - BLAST (generating tests from counterexamples)
# - (Target)

#+BEGIN_ABSTRACT
Program testing is a crucial yet tedious part of software development. The
standard /unit-testing/ practice dictates manual specification of input-output
pairs for each expected behavior of a program, which involves large amounts of
programmer effort and does not scale well with the size of a program. As a
result, many groups have investigated techniques for automatically generating
test-cases from a specification or directly from the source code.

We survey a selection of the prominent approaches for automatic test-case
generation and present our own contribution to the field, a technique for
generating inputs to highly constrained functions by specifying the expected
behavior via /refinement types/.
#+END_ABSTRACT

* Introduction
There are two core questions an automatic test-case generator must
answer:
1. How do we generate input values?
2. How do we determine the correctness of an execution?

In general, the answer for (2) involves checking that the execution
satisfies some property, e.g. crash-freedom. As such, we will primarily
categorize systems by their answer to (1).
* A Running Example? :noexport:
#+BEGIN_SRC haskell
data Tree a 
  = Leaf
  | Node a (Tree a) (Tree a)

insert :: Ord a => a -> Tree a -> Tree a
#+END_SRC
* Enumerating Inputs
Perhaps the simplest method of automatically testing a program is to enumerate
valid inputs based on its external interface -- an instance of /black-box
testing/ \cite{adrion_validation_1982} -- and check whether the program behaves
correctly on these inputs. Of course, enumerating /all/ inputs is generally
infeasible, so we must find some way of narrowing the search space. The two
common solutions are enumerating /small/ inputs and random sampling of the
entire space.

** Enumerating "small" inputs
The /small-scope hypothesis/ \cite{jackson_software_2006} argues that if
a property is invalid, there is likely a small counterexample, i.e. if a
program contains a bug there is likely a small input that will trigger
it. Thus, we can restrict our enumeration to "small" inputs and still
gain a large degree of confidence in our program.

*** SmallCheck
SmallCheck \cite{runciman_smallcheck_2008} is a testing library for Haskell
programs that does the simplest thing possible. Programmers specify how to
enumerate input values using Haskell's type-class mechanism
\cite{wadler_how_1989} and provide a boolean-valued function that describes the
property they wish to check, and SmallCheck enumerates all possible inputs up to
some depth, validating the property on each input vector. The input generators
are expected to produce values in increasing size (indeed, such a generator can
be automatically derived from a datatype definition), thus SmallCheck is
guaranteed to find the /minimal/ counterexample if it lies within the
depth-bound.

Naturally, such a brute-force enumeration of input vectors is bound to be
wasteful (i.e. it will produce many inputs that trigger the same code path),
thus the authors also introduce a /lazy/ variant that takes advantage of
Haskell's inherent laziness to prune the search space. The key distinction of
Lazy SmallCheck is that it produces /partially-defined/ values instead of
fully-defined values, i.e. Lazy SmallCheck will initially set each
component of a value with $\bot$. Only when a value is demanded (and Haskell's
runtime throws an exception) will Lazy SmallCheck fill in the "hole" with an
actual value. Thus, Lazy SmallCheck reduces the search space by dynamically
discovering which parts of a value are relevant to the property being tested.

Initially, Lazy SmallCheck had several disadvantages compared to SmallCheck. It
could not generate functional values (commonly used in Haskell), and it could
not print the partially-defined counterexample, instead it would have to
concretize the input vector, thus /losing/ information. These were both added
later as enhancements \cite{reich_advances_2013}.  A remaining concern is that
programmers must be careful about the order in which they conjoin predicates,
giving precedence to the lazier predicate. For example, a binary-search tree
implementation will want to check the ordering invariant before balancedness;
the former will often fail within the first few nodes whereas the latter will
force the entire spine.

*** TestEra / Korat
TestEra \cite{khurshid_testera:_2004,marinov_testera:_2001} provides (bounded)
exhaustive testing of Java methods against an Alloy
\cite{jackson_automating_2000} specification. Unlike SmallCheck, which
enumerates input vectors by unfolding each possible data constructor and
enumerating its arguments, TestEra begins with a pre-defined universe of atomic
values (e.g. primitive types and uninitialized objects), and uses the Alloy
Analyzer \cite{jackson_alcoa:_2000} to enumerate all valid /relations/ between
the atoms, i.e. setting the pointers appropriately such that the method's
preconditions are satisfied. Thus, an important optimization that TestEra
implements is filtering /isomorphic/ inputs. For each symbolic input vector
produced by Alloy, TestEra /concretizes/ the input to a set of Java objects,
executes the method, and then /abstracts/ the result back to an Alloy
value. Finally, it queries the Alloy Analyzer again to determine whether the
output satisfies the method's postcondition, then proceeding to the next input
vector. Unlike SmallCheck, TestEra is not guaranteed to find a minimal
counterexample as the testing order is left up to Alloy.

Korat \cite{boyapati_korat:_2002} is a rewrite of TestEra that uses a custom
enumeration algorithm instead of calling out to Alloy. Korat enumerates all
input candidates in lexicographic order, and runs an instrumented version of a
programmer-supplied =repOk= method (to check class invariants and preconditions)
that tracks field accesses. If =repOk= returns /true/ Korat immediately outputs
all (non-isomorphic) input vectors that share the current valuation of the
/accessed/ fields, as the valuation of the unread fields cannot have affected
the outcome of =repOk=. More importantly, if =repOk= returns /false/ Korat
immediately backtracks, skipping all variant candidates that share the current
valuation of the accessed fields, as /none/ of them can be valid. Thus, Korat
takes advantage of the inherent laziness of invariant-checking predicates to
quickly prune the search space of invalid inputs, in a very similar manner to
Lazy SmallCheck.

# TestEra \cite{khurshid_testera:_2004,marinov_testera:_2001} and Korat
# \cite{boyapati_korat:_2002} both provide (bounded) exhaustive testing of Java
# methods. Given a method, a formal specification of the desired behavior, and a
# universe of atomic values (e.g. primitive values and uninitialized objects),
# TestEra and Korat begin by enumerating all valid instantiations of the 

** Random sampling of inputs
The obvious drawback to the approaches described above is that they will not
detect bugs that only present on "large" inputs. Thus, a common alternative to
enumeration of small inputs is random selection of input vectors from the entire
search space.

*** QuickCheck
QuickCheck \cite{claessen_quickcheck:_2000,claessen_testing_2002} enables
randomized testing of Haskell programs by providing an embedded domain-specific
language for generating /arbitrary/ values of a given datatype. As with
SmallCheck, QuickCheck properties are boolean-valued Haskell functions whose
inputs can be (randomly) generated, and the input-generators (usually) operate
by unfolding a specific data constructor and generating sub-values for the
constructor's fields. Unlike SmallCheck it is impractical to automatically
derive QuickCheck generators for datatypes, as one must take care to ensure the
generator covers a uniform distribution of values. For example, a generator for
a simple "list" type

#+BEGIN_SRC haskell
data List a = Nil | Cons a (List a)
#+END_SRC

that chooses between =Nil= and =Cons= with equal probability is highly unlikely
to generate lists with more than a handful of elements. A further concern
arising from random testing is that the returned counterexample may be quite
large, as demonstrated by Pike \cite{pike_smartcheck:_2014}. Thus, subsequent
iterations of QuickCheck introduced support for /shrinking/ counterexamples
\cite{hughes_quickcheck_2006}. Once QuickCheck has found a counterexample it
will invoke a user-defined =shrink= function on the input vector, which will
return a list of smaller inputs. QuickCheck will test each small candidate in
turn and repeat the shrinking process on any new counterexamples, finally
returning the smallest counterexample it could find. Notably, the shrinking
process is not guaranteed to find a /minimal/ counterexample.

Pike \cite{pike_smartcheck:_2014} builds on top of QuickCheck with SmartCheck,
which provides automatically-derivable shrinking definitions that are shown to
perform favorably compared to handwritten =shrink= functions, and produce
smaller counterexamples. More interestingly, SmartCheck also introduces
/counterexample generalization/, which attempts to produce a universal property
describing a class of counterexamples. For example, Pike shows that SmartCheck
can reduce a large counterexample like

#+BEGIN_SRC haskell
StackSet {current = Screen {workspace = Workspace
{tag = NonNegative {getNonNegative = 0}, layout =
-1, stack = Just (Stack {focus = ‘S’, up ="", down
= ""})}, screen = 1, screenDetail = 1}, visible =
[Screen {workspace = Workspace {tag = NonNegative
{getNonNegative = 2}, layout = -1, stack =
Nothing}, screen = 2, screenDetail = -1},Screen
{workspace = Workspace {tag = NonNegative
{getNonNegative = 3}, layout = -1, stack =
Nothing}, screen = 0, screenDetail = -1}], hidden
= [Workspace {tag = NonNegative {getNonNegative =
1}, layout = -1, stack = Just (Stack {focus =
‘NUL’, up = "", down = ""})}, Workspace {tag =
NonNegative {getNonNegative = 4}, layout = -1,
stack = Just (Stack {focus = ‘I’, up = "", down =
""})}], floating = fromList []}
#+END_SRC

to a comparatively simple formula

#+BEGIN_SRC
forall values x0 x1 x2 x3:
  StackSet
    (Screen (Workspace x0 (-1) (Just x1)) 1 1)
    x2 x3 (fromList [])
#+END_SRC

thus abstracting away the irrelevant portions of the counterexample. The
key insight is that if one can replace a sub-value by another arbitrary
value without affecting the test outcome, then the sub-value must not
affect the outcome. Thus, SmartCheck systematically replaces all
sub-values of the counterexample with other random values and
generalizes the counterexample accordingly.

QuickCheck does not currently have good support for testing properties
with preconditions, due to the low probability of randomly generating a
value that satisfies the precondition. \cite{claessen_generating_2014}
describes an algorithm for random generation of constrained inputs
based on \cite{durega_ard_feat:_2012}, by defining a function to index
into a uniform distribution of constrained values, and then generating
random indices, but the work has not yet been incorporated into
QuickCheck.

*** JCrasher
*** Randoop
** Limitations :noexport:
*** Preconditions
- need for programmer intervention to specify "smart" generators
  - or fall back to generate-and-filter approach
  - can be mitigated to some extent by lazy construction of inputs
* Enumerating Code Paths
The drawback to explicit enumeration of input vectors is that many inputs will
trigger similar behavior in the program under test. Indeed unit testing texts
often advise programmers to first partition program inputs into /equivalence
classes/, and then test a single input vector from each equivalence class,
thereby minimizing the number of handwritten tests required
\cite{burnstein_practical_2003}. So instead of enumerating inputs, perhaps we
should enumerate program behaviors, i.e. paths through the program. This
necessarily requires knowledge of the internal structure of the program under
test, thus tools that take this approach will fall in the category of /white-box
testing/ \cite{adrion_validation_1982}.

Tools that take this approach typically use /dynamic-symbolic execution/, which
combines traditional symbolic execution with concrete execution, to quickly
explore different paths through the program. The two main categories of
dynamic-symbolic execution-based testing tools are concolic testing and
execution-generated testing, both introduced independently in 2005
\cite{godefroid_dart:_2005,cadar_execution_2005}.

** Symbolic Execution
Symbolic execution as a method of testing programs is not a new idea, it was
introduced in 1976 by King \cite{king_symbolic_1976}. The key difference in
between symbolic and concrete execution is that instead of mapping program
variables to /values/, a symbolic executor maps them to /symbolic
expressions/. For example, given the simple program

#+BEGIN_SRC c
int f (int x, int y) {
  return 2 * (x + y);
}
#+END_SRC

a concrete execution may begin with input vector $\{x \mapsto 1, y \mapsto 2\}$
and return $6$. A symbolic execution, however, will begin with an input vector
$\{x \mapsto \alpha_1, y \mapsto \alpha_2\}$ -- where $\alpha_i$ are symbolic
variables -- and return $2 * (\alpha_1 + \alpha_2)$, thereby precisely
describing /all/ possible executions of ~f~.

Another key difference of symbolic execution is its handling of
conditionals. Consider the first conditional in the following program.

#+BEGIN_SRC c
int f (int x) {
  if (x > 0) {
    if (x == 0) {
      abort();
    }
  }
  return 0;
}
#+END_SRC

With the input vector $\{x \mapsto \alpha_1\}$, the symbolic executor does not
know which direction of the branch it should take, as it knows nothing about the
symbolic variable $\alpha_1$. Therefore it must follow both directions! When
following a branch, the symbolic executor records the symbolic expression
associated with the chosen direction in its /path constraint/, which we will
write as a sequence of expressions $\langle e_1, e_2, \ldots \rangle$. For example, in
the outer conditional above, the "true" case would record $\langle \alpha_1 > 0
\rangle$ and the false case would record $\langle \lnot (\alpha_1 > 0)
\rangle$. Thus, it remembers what properties of the program inputs will trigger
specific paths through the code. When the symbolic executor reaches a branch
point, it consults the current path constraint to determine with directions are
feasible. For example, upon reaching the inner conditional above, the symbolic
executor will check whether $\alpha_1 = 0$ is consistent with the path condition
$\langle \alpha_1 > 0 \rangle$, i.e. is the formula $\alpha_1 = 0\ \land\ \alpha_1
> 0$ satisfiable? As the formula is clearly unsatisfiable, the symbolic executor
will decide that the "true" branch is /unreachable/, and continue by only pursuing
the "false" branch. Thus, a symbolic executor can statically determine that the
~abort()~ call above can /never/ be executed.

While a powerful idea in theory, symbolic execution crucially relies on a
theorem prover to solve the symbolic expressions it creates, and as such it went
relatively unused until recent advances in constraint solving technology.

** Concolic Testing
Godefroid et al. introduced /concolic testing/ in 2005
\cite{godefroid_dart:_2005}. Concolic testing performs symbolic and concrete
execution of a program in tandem. Thus, when confronted with a program expression
that the symbolic executor cannot reason about, a concolic tester can fall back
to the concrete value and continue execution with more precision than a purely
symbolic approach.

*** DART
DART \cite{godefroid_dart:_2005} instruments a C program to execute
each instruction both concretely and symbolically, then performs a
depth-first search of all paths through the program, starting
with a random input vector. At each branch point, DART records the branch
condition and the direction taken, thereby building a /path
constraint/. For example, suppose DART is testing the following C program
with initial inputs $\{x \mapsto 5, y \mapsto 6\}$.

#+BEGIN_SRC c
int f (int x, int y) {
  if (x == 5) {
    if (2 * y == x) {
      abort();
    }
  }
  return 0;
}
#+END_SRC

This execution will satisfy $x = 5$ but not $2y = x$, thus the path
constraint will be $\langle x = 5,\ 2y \neq x \rangle$. Next, DART will
negate the last (right-most) predicate in the path constraint and query
a constraint solver for a solution to $x = 5 \land 2y = x$, in order to
produce a new input vector.  There is only one solution to this
constraint, $\{x \mapsto 5, y \mapsto 10\}$, which will force execution
through the /true/ branch of both conditionals, right into the erroneous
=abort()= call. Since the concrete execution reached the =abort()= call,
we know it is a real bug as opposed to a false positive that could come
from a purely symbolic approach, i.e. DART /soundly/ reports bugs.

When confronted with an expression that it cannot reason about
symbolically, e.g. multiplication of two variables or a dereference of a
pointer that depends on program input, DART will fall back to recording
the result of the concrete evaluation. For example, given

# #+BEGIN_SRC c
# struct foo { int i; char c; }
# bar (struct foo *a) {
#   *((char *)a + sizeof(int)) = 1;
#   if (a->c != 1)
#     abort();
# }
# #+END_SRC

# and input vector $\{a \mapsto \{i = 0, c = 0\}\}$, DART will be unable to
# symbolically execute the first statement, so it will just record the concrete
# result, which sets =a->c= to the constant $1$. In the following conditional 

#+BEGIN_SRC c
int f (int x, int y) {
  if (x == y*y) {
    abort();
  }
  return 0;
}
#+END_SRC

and starting inputs $\{x \mapsto 5, y \mapsto 2\}$, DART will produce a
path constraint $\langle x \neq 4 \rangle$ for the first
execution. Refuting this path constraint will /not/ produce an input
vector that is guaranteed to take the /true/ branch -- indeed the solver
may return the original input vector -- thus DART suffers a severe loss
of precision when the path-constraint veers outside the language of the
constraint solver. In effect, this means DART degenerates to brute-force
enumeration of inputs, as in Sec [[Enumerating Inputs]].

Furthermore, DART's depth-first enumeration of paths means that it may
fail to discover all paths when presented with recursive programs,
e.g. a program that checks the ordering invariant of a binary-search
tree. In this case DART will loop forever, generating increasingly deep
trees whose right sub-trees are always =NULL= (assuming the program
checks the left sub-tree first).

*** CUTE
Sen et al. introduced CUTE \cite{sen_cute:_2005} later that year, an extension
of DART that adds support for testing complex datatypes. CUTE enhances DART's
technique by adding support for (dis)equality constraints on pointers, and by
switching to a /bounded/ depth-first search.

**** Pointer (dis)equality
Whereas DART maintained a single map of memory locations to symbolic arithmetic
expressions, CUTE maintains two maps of memory locations: (1) $\mathcal{A}$ to
arithmetic expressions and (2) $\mathcal{P}$ to pointer expressions. $\mathcal{A}$
contains the usual linear arithmetic expressions as in DART; however,
$\mathcal{P}$ contains expressions of the form $x_p \cong y_p$ where
$x_p$ is either a symbolic variable or the constant symbol =NULL= and 
$\cong\ \in \{=, \neq\}$. When solving a pointer constraint, CUTE partitions the
variables in $\mathcal{P}$ into equivalence classes and applying the arithmetic
constraints to all members of the equivalence class. For example, given

#+BEGIN_SRC c
int f (int *x, int *y) {
  if (x == y) {
    if (*x == 5) {
      return 0;
    }
  }
  return 0;
}
#+END_SRC

and the path constraint $\langle x = y,\ *x \neq 5 \rangle$, when CUTE refutes
the $*x \neq 5$ conjunct, the value of $*y$ will /also/ be forced to $5$ as $x$
and $y$ are in the same equivalence class.

**** Bounded Depth-First Search
In order to avoid an infinite loop from the repeated inlining of a loop body or
recursive call, CUTE places a configurable bound $k$ on the number of predicates
in the path constraint. Once the path constraint is full, CUTE stops recording
any further nested branch conditions, thereby forcing the refutation process to
negate an earlier constraint. For example, given

#+BEGIN_SRC c
int f (int n) {
  for (int i = 0; i < n; i++ ) {
    ...
  }
  return 0;
}
#+END_SRC

and $k = 4$, CUTE will never force more than four iterations of the
loop body, as the path constraint will be cut off at
$\langle i_0 < n,\ i_1 < n,\ i_2 < n,\ i_3 < n \rangle$. Negating the last conjunct
will force $n \leq 3$, and CUTE will begin to backtrack through the path
constraint until it terminates. While this tactic forces broad rather than deep
coverage, it also means that CUTE may miss bugs deep in the execution graph of
the program, e.g. if the loop body above were ~if (i == 5) abort();~.

Another tactic CUTE employs to quickly achieve high coverage is branch
prediction. Since CUTE only refutes the final conjunct of the path constraint,
the outcomes of the previous branches should remain the same. Deviation from the
previous path at an earlier branch indicates an imprecision in the symbolic
executor; in this case CUTE will decide to restart execution with random inputs
instead of allowing the loss of precision.

**** Notes :noexport:
Two main improvements over DART:
1. handles (dis)equality constraints on pointers, whereas DART pointers
   were either =NULL= or non-=NULL=.
2. Bounded Depth-first Search: restricts length of path constraint.
   remembers evaluation of conditionals from
   previous execution. if any conditional evaluates differently from
   last execution (prediction), throw exception to restart with
   randomized inputs. intuition is that failed prediction implies some
   imprecision in constraints

*** PEX
Tillman and Halleaux further extended concolic testing with Pex
\cite{tillmann_pexwhite_2008} in 2008, adding heuristics to improve
path-selection, modeling of interactions with the environment, and a richer
constraint language.

**** Richer constraints
Whereas previous systems had limited constraint languages -- linear arithmetic
for DART, with the addition of pointer equality for CUTE -- Pex takes advantage
of the rich constraint language offered by Z3 \cite{de_moura_z3:_2008}. Pex
supports linear arithmetic, bit-vectors, arrays directly via Z3. Pex further
supports floating-point numbers with an approximation to rational numbers.

**** Improving path-selection
Instead of performing a depth-first search of all program paths, Pex maintains a
tree of all branch conditions it has encountered. After exploring a path, Pex
will choose a new unexplored path from the unexplored leaves of the execution
tree, using several heuristics to partition branches into equivalence classes
and then choosing a new branch from the least-often chosen class. Thus, Pex
favors a more breadth-oriented search than DART or CUTE, while avoiding
randomness in its path-selection.

**** Dealing with the environment
Pex builds a model of the environment by recording the inputs and outputs of
function calls where the source code is unavailable. This allows Pex to increase
its precision when determining the feasibility of a path, but it also makes Pex
unsound as the model is necessarily an under-approximation.

**** Notes :noexport:
- collection of heuristics for finding new paths
  - maintains execution tree of explored branches
  - picks a new unexplored branch from set of all known unexplored branches
- handles environment
  - builds model of environment based on actual inputs and outputs
  - (under-approximation)
- extends constraint language with
  - floats
  - arrays
- inserts checks for potentially unsafe operations, e.g. array index
** Execution-Generated Testing
Instead of performing symbolic and concrete execution in tandem,
/execution-generated testing/ \cite{cadar_execution_2005} begins with pure
symbolic execution and lazily generates concrete inputs on demand. When a
dangerous operation (e.g. division or memory read/write) is about to be
executed, the system will insert an implicit branch denoting the possibility of
an error (e.g. divide-by-zero or out-of-bounds write). If the error branch is
deemed feasible, the system will then solve the path constraint for an input
vector designed to trigger the error condition. Similarly, function calls into
uninstrumented code, e.g. library functions or system calls, will induce a call
to the constraint solver for a concrete set of inputs designed to trigger the
call. When the external call returns, the system will continue execution with
the concrete result, thus improving precision over pure-symbolic approaches that
would have to somehow model the interaction with the external world (often
simply assuming nothing about the result).

*** EXE
Cadar et al. introduced execution-generated testing with EXE
\cite{cadar_exe:_2006}. EXE models program memory as arrays of bitvectors,
enabling bit-precise reasoning about the C programs it tests via the
co-developed constraint solver STP \cite{ganesh_decision_2007}. This crucial
distinction from DART and CUTE allows EXE and STP to view program values in the
same way as the systems software they test, as untyped bytes.

At each branch EXE forks execution for each direction of the branch that is
deemed feasible. The child processes add their direction to the path contraint
and go to sleep. A master process then decides which child (path) should
continue executing, using a combination of depth-first and best-first
search. The master process chooses the child blocked on the instruction with the
lowest execution count and runs it and its children in DFS for some period of
time. Then it picks another best candidate and repeats the process.

An important optimization of EXE is /aggressive concretization/. If the operands
are all concrete (i.e. constant values), EXE will simply perform the operation
and record the resulting concrete value. This helps simplify the queries sent
to STP, such that the only symbolic variables in a query will have a data
dependence on one of the initial symbolic variables.

**** EXE Notes                                                     :noexport:
- "bit-precise" handling of memory
  - models memory as array of bitvectors, untyped
- custom developed constraint solver STP, optimized for bitvector queries
  - bit-blast to propositional logic formula, send to SAT solver
- symbolically executes code
  - builds path condition a la CUTE
  - concrete execution when all operands are concrete values
- queries constraint solver at branch
  - pursues feasible directions
- "dangerous" operations (div-zero, load/store) induce implicit branches
  with one direction throwing ERROR
- handles pointer aliasing by forking execution for each possible reference
- if ERROR (or EXIT) detected, generate test case to trigger path
- combination of DFS and best-first search
  - chooses path whose current LoC has been hit fewest time, runs path/children
    in DFS for a while, repeat
- aggressive concretization
  - user must mark inputs as symbolic
  - everything else assumed concrete
  - if both operands are concrete, just perform the operation concretely

*** KLEE
In 2008, Cadar et al. rewrote EXE as KLEE \cite{cadar_klee:_2008}, which
symbolically executes LLVM IR \cite{lattner_llvm:_2004} and provides several
enhancements over EXE.

**** Compact process representation
Whereas EXE processes relied on the host OS to share memory and was thus limited
to page-level granularity, KLEE implements sharing with a granularity of
individual objects, thus tracking many more processes than EXE could with the
same memory limit. This optimization enabled KLEE to scale up to testing all of
GNU Coreutils.

**** Random path selection and Coverage-optimized search
KLEE employs two path selection strategies in round robin to prevent either one
from getting stuck. /Random path selection/ maintains a tree of all branches
KLEE has encountered. It starts at the root and randomly picks a child node
until it hits a leaf, and schedules the corresponding process for
execution. This favors broad and shallow coverage, while still allowing for deep
paths to be chosen. /Coverage-optimized search/ weights each process according
to some heuristics, e.g. distance to an unexecuted instruction, and biases the
choice accordingly.
**** Environment modeling
KLEE models the environment at the level of system calls, by replacing the
actual system call with a simplified C implementation. Thus there is no
"foreign" code and the developers can model interactions with the external world
with as much precision as they desire. The drawback is that KLEE must now
additionally reason about the mock system calls (as well as any library code
leading up to them).

**** KLEE Notes                                                    :noexport:
- rewrite of EXE with enhancements
- symbolically executes llvm IR
- /random path selection/: tree of program paths, start at root and
  randomly pick subtree until a leaf is found.
- models system calls with simple C implementations (sounds like an
  under-approximation)
** CREST? :noexport:
** Limitations :noexport:

**** Path explosion

**** Modeling the environment
* Tests from Counterexamples
In the previous section we discussed approaches whose aim was to achieve high
program coverage, i.e. to execute as many instructions as possible in a short
period. However even this may seem wasteful in the presence of tools that can
/prove/ a program correct. 

Program verification is the process of analyzing a program and constructing a
formal proof that it satisfies some correctness condition. As before we will use
crash-freedom as our correctness condition, as high-level safety properties can
be rewritten in terms of crash-freedom. A verifier is considered /sound/ if it
never reports a false positive, i.e. if the verifier claims your program is
bug-free, it truly is. The converse does not generally hold; even if your
program is bug-free the verifier may still report a possible bug, as it often
has to /over-approximate/ program behavior in order to achieve soundness. For
instance, many verifiers struggle with non-linear arithmetic, i.e. they would be
unable to verify

#+BEGIN_SRC c
int f (int x, int y) {
  if (x > 0 && y > 0) {
    return 1 / (x * y);
  }
}
#+END_SRC

because the underlying theorem prover cannot handle multiplication of two
variables. Thus, when a verifier reports a potential bug, the programmer must
still manually inspect the verifier's output to determine if the bug is genuine
or fictitious. Luckily, many theorem provers produce a counterexample when
verification fails. The insight of the tools we discuss in this section is that
these counterexamples can be transformed into concrete test cases designed to
trigger the erroneous behavior. Thus, one only need test the paths that cannot
be statically proven safe.

** Check'n'Crash :ignoreheading:
Check'n'Crash \cite{csallner_check_2005} builds on top of JCrasher and the
ESC/Java contract checker \cite{flanagan_extended_2002}. It runs ESC/Java on the
supplied program and then solves the constraint system arising from a
counterexample for concrete program input. Check'n'Crash can solve constraints
involving integer arithmetic, object aliases, and multidimensional arrays, and
can always fallback to the purely random testing of JCrasher if it cannot solve
the constraint system. It then uses JCrasher to automatically generate test
methods from the solutions. Note that a counterexample may assign program
variables to symbolic expressions instead of concrete values, e.g. in the above
the counter example would be $x > 0 \land y > 0$, thus Check'n'Crash must
enumerate all possible solutions to the counterexample to be sure the bug does
not exist.

** DSD-Crasher :ignoreheading:
DSD-Crasher \cite{csallner_dsd-crasher:_2008} extends Check'n'Crash by first
running the Daikon \cite{ernst_dynamically_2001} invariant detection tool on the
program. The inferred invariants are translated into JML specifications so that
ESC/Java can digest them and avoid paths that would be triggered by invalid
inputs. Thus, DSD-Crasher is able to generate test-suites with fewer false
positives than Check'n'Crash, as it infers the programmer's intent. The
drawback, however, is that Daikon requires a sizeable test-suite to infer
precise invariants, so the prospective user of DSD-Crasher is left with
something of a chicken-and-egg problem. 

**** Notes :noexport:
- integrates Daikon as a first step in the check'n'crash formula to infer likely
  invariants and improve precision of esc/java

** BLAST :ignoreheading:
Beyer et al. \cite{beyer_generating_2004} take a slightly different approach,
using the BLAST \cite{beyer_software_2007} model-checker to generate test
vectors that drive execution to each location where a user-supplied predicate
$p$ holds. They use BLAST to translate a C program into a control-flow
automaton, which it then traverses to generate all traces that satisfy $p$ at
the final location. These traces are sequences of assignments and assumptions
about the program state (e.g. from taking a specific direction of a branch), and
must be converted into concrete test vectors before they can be executed. BLAST
then translates these traces into logical formulae encoding constraints on the
program variables and queries a theorem prover for a satisfying assignment,
which finally represents a concrete test vector.

An advantage of this approach over the Check'n'Crash approach is that the user
can supply any predicate they wish and BLAST will find states that satisfy it,
whereas Check'n'Crash will only find states that ESC/Java deems unsafe. (One
could insert explicitly failing assertions in specific locations to guide
Check'n'Crash, but this is more work for the user.)
**** Notes :noexport:

** Reach? :noexport:
\cite{naylor_finding_2007}
* Type-Targeted Testing
- /Filtered/ enumeration of inputs


#+LATEX: \bibliographystyle{plain}
#+LATEX: \bibliography{bibliography}
* Future Work?                                                     :noexport:
- modeling environment still a problem
- many tools to /detect/ bugs, what about fixing them?
  - see Weimer
- or understanding them?
  - counterexamples reported by verifiers are pretty painful to work with
  - concrete test vector helps, but path from input vector to bug may be complex
    - perhaps combine test-from-counterexample with program-slicing to highlight
      relevant path?
    - see jhala path-slicing
